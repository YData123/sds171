{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 07: Abstracts of Scientific Articles (2/2)\n",
    "In this lab we will continue working with topic models, but this time with a new dataset. Instead of abstracts of scientific articles, we will create topic models over movie plot descriptions. This is a dataset containing descriptions of movies from Wikipedia. The dataset was [obtained](https://www.kaggle.com/jrobischon/wikipedia-movie-plots) from Kaggle, an online community of data scientists.\n",
    "\n",
    "We will review the Counter class and how to build vocabularies again, and get practice with building topic models for new datasets. We'll also introduce the concept of stemming and lemmatization. \n",
    "\n",
    "Spoiler alert for the movie \"[Husbands and Wives](https://en.wikipedia.org/wiki/Husbands_and_Wives)\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datascience import *\n",
    "import numpy as np\n",
    "import re\n",
    "import gensim\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "logging.root.level = logging.CRITICAL \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "\n",
    "# direct plots to appear within the cell, and set their style\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plots\n",
    "plots.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time round, the movie plot descriptions are in a CSV format in `movie_plots.csv`. The file is hosted on the Amazon Web Service s3. We'll use the `datascience` package to read this CSV file.\n",
    "\n",
    "The CSV file is rather large, so we'll randomly sample only 10000 rows, skipping over the others during the loading process. We could have sampled the rows after loading the entire CSV file, but this method saves memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Release Year</th> <th>Title</th> <th>Origin/Ethnicity</th> <th>Director</th> <th>Cast</th> <th>Genre</th> <th>Wiki Page</th> <th>Plot</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>1901        </td> <td>Kansas Saloon Smashers       </td> <td>American        </td> <td>Unknown           </td> <td>nan      </td> <td>unknown</td> <td>https://en.wikipedia.org/wiki/Kansas_Saloon_Smashers        </td> <td>A bartender is working at a saloon, serving drinks to cu ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1901        </td> <td>Love by the Light of the Moon</td> <td>American        </td> <td>Unknown           </td> <td>nan      </td> <td>unknown</td> <td>https://en.wikipedia.org/wiki/Love_by_the_Light_of_the_Moon </td> <td>The moon, painted with a smiling face hangs over a park  ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1901        </td> <td>The Martyred Presidents      </td> <td>American        </td> <td>Unknown           </td> <td>nan      </td> <td>unknown</td> <td>https://en.wikipedia.org/wiki/The_Martyred_Presidents       </td> <td>The film, just over a minute long, is composed of two sh ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1903        </td> <td>Alice in Wonderland          </td> <td>American        </td> <td>Cecil Hepworth    </td> <td>May Clark</td> <td>unknown</td> <td>https://en.wikipedia.org/wiki/Alice_in_Wonderland_(1903_ ...</td> <td>Alice follows a large white rabbit down a \"Rabbit-hole\". ...</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1904        </td> <td>The Suburbanite              </td> <td>American        </td> <td>Wallace McCutcheon</td> <td>nan      </td> <td>comedy </td> <td>https://en.wikipedia.org/wiki/The_Suburbanite               </td> <td>The film is about a family who move to the suburbs, hopi ...</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>\n",
       "<p>... (9995 rows omitted)</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(101)\n",
    "\n",
    "total_size = 34886\n",
    "sample_size = 10000\n",
    "sample_rows = np.random.choice(range(1,total_size+2), sample_size, replace=False)\n",
    "skip_rows = np.setdiff1d(range(1,total_size+2), sample_rows)\n",
    "\n",
    "filename = \"https://s3.amazonaws.com/sds171/labs/lab07/movie_plots.csv\"\n",
    "data = Table.read_table(filename, skiprows=skip_rows)\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now in the `Table` format, which we are familiar with. We are only concerned with the plots, so we will extract them from the `Table`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 10000\n",
      "\n",
      "The film is about two couples: Jack (Pollack) and Sally (Davis), and Gabe (Allen) and Judy (Farrow). The film starts when Jack and Sally arrive at Gabe and Judy's apartment and announce their separation. Gabe is shocked, but Judy takes the news personally and is very hurt. Still confused, they go out for dinner at a Chinese restaurant.\r\n",
      "A few weeks later Sally goes to the apartment of a colleague. They plan to go out together to the opera and then to dinner. Sally asks if she can use his phone, and calls Jack. Learning from him that he has met someone, she accuses him of having had an affair during their marriage.\r\n",
      "Judy and Gabe are introduced to Jack's new girlfriend, Sam, an aerobics trainer. While Judy and Sam shop, Gabe calls Jack's new girlfriend a \"cocktail waitress\" and tells him that he is crazy for leaving Sally for her. About a week later, Judy introduces Sally to Michael (Neeson), Judy's magazine colleague who she clearly is interested in herself. Michael asks Sally out, and they begin dating; Michael is smitten, but Sally is dissatisfied with the relationship.\r\n",
      "Meanwhile, Gabe has developed a friendship with a young student of his, Rain, and has her read the manuscript of his novel. She comments on its brilliance, but has several criticisms, to which Gabe reacts defensively.\r\n",
      "At a party, Jack learns from a friend that Sally is seeing someone, and flies into a jealous rage. He and Sam break up after an intense argument, and Jack drives back to his house to find Sally in bed with Michael. He asks Sally to give their marriage another chance, but she tells him to leave.\r\n",
      "Less than two weeks later, however, Jack and Sally are back together and the couple meet Judy and Gabe for dinner like old times. After dinner, Judy and Gabe get into an argument about her not sharing her poetry. After Gabe makes a failed pass at her, Judy tells him she thinks the relationship is over; a week later Gabe moves out. Judy begins seeing Michael.\r\n",
      "Gabe goes to Rain's 21st birthday party, and gives her a music box as a present. She asks him to kiss her, and though the two share a romantic moment, Gabe tells her they should not pursue it any further. As he walks home in the rain, he realizes that he has ruined his relationship with Judy.\r\n",
      "Michael tells Judy he needs time alone, then says he can't help still having feelings for Sally. Angry and hurt, Judy walks out into the rain. Highlighting her \"passive aggressiveness,\" Michael follows and begs her to stay with him. A year and a half later they marry.\r\n",
      "At the end, the audience sees a pensive Jack and Sally back together. Jack and Sally admit their marital problems still exist (her frigidity is not solved), but they find they accept their problems as simply the price they have to pay to remain together.\r\n",
      "Gabe is living alone because he says he is not dating for the time being, as he does not want to hurt anyone. The film ends with an immediate cut to black after Gabe asks the unseen documentary crew, \"Can I go? Is this over?\"\n"
     ]
    }
   ],
   "source": [
    "sample = 3402\n",
    "\n",
    "plots = data.column('Plot')\n",
    "    \n",
    "print(\"Number of documents: %d\\n\" % len(plots))\n",
    "print(plots[sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot description is from the movie \"[Husbands and Wives](https://en.wikipedia.org/wiki/Husbands_and_Wives)\"\n",
    "\n",
    "We don't have LaTeX markup in these documents, so we can use regex to do some simpe pre-processing of punctuation. There are lots of names in the plot descriptions, so we'll remove all the words that have a capitalized first letter. This will remove lots of non-name words as well, but this'll be sufficient for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " film is about two couples   and   and   and    film starts when  and  arrive at  and  apartment and announce their separation  is shocked but  takes the news personally and is very hurt  confused they go out for dinner at a  restaurant\r\n",
      " few weeks later  goes to the apartment of a colleague  plan to go out together to the opera and then to dinner  asks if she can use his phone and calls   from him that he has met someone she accuses him of having had an affair during their marriage\r\n",
      " and  are introduced to  new girlfriend  an aerobics trainer   and  shop  calls  new girlfriend a cocktail waitress and tells him that he is crazy for leaving  for her  a week later  introduces  to    magazine colleague who she clearly is interested in herself  asks  out and they begin dating  is smitten but  is dissatisfied with the relationship\r\n",
      "  has developed a friendship with a young student of his  and has her read the manuscript of his novel  comments on its brilliance but has several criticisms to which  reacts defensively\r\n",
      " a party  learns from a friend that  is seeing someone and flies into a jealous rage  and  break up after an intense argument and  drives back to his house to find  in bed with   asks  to give their marriage another chance but she tells him to leave\r\n",
      " than two weeks later however  and  are back together and the couple meet  and  for dinner like old times  dinner  and  get into an argument about her not sharing her poetry   makes a failed pass at her  tells him she thinks the relationship is over a week later  moves out  begins seeing \r\n",
      " goes to  21st birthday party and gives her a music box as a present  asks him to kiss her and though the two share a romantic moment  tells her they should not pursue it any further  he walks home in the rain he realizes that he has ruined his relationship with \r\n",
      " tells  he needs time alone then says he cant help still having feelings for   and hurt  walks out into the rain  her passive aggressiveness  follows and begs her to stay with him  year and a half later they marry\r\n",
      " the end the audience sees a pensive  and  back together  and  admit their marital problems still exist her frigidity is not solved but they find they accept their problems as simply the price they have to pay to remain together\r\n",
      " is living alone because he says he is not dating for the time being as he does not want to hurt anyone  film ends with an immediate cut to black after  asks the unseen documentary crew   go  this over\n"
     ]
    }
   ],
   "source": [
    "# replace '-' with ' ', then remove punctuation\n",
    "plots = [re.sub(r'-', ' ', plot) for plot in plots]\n",
    "plots = [re.sub(r'[^\\w\\s]', '', plot) for plot in plots]\n",
    "\n",
    "# remove tokens with a capitalized first letter \n",
    "# (broad stroke to remove names)\n",
    "plots = [re.sub(r'[A-Z]\\w*', '', plot) for plot in plots]\n",
    "\n",
    "print(plots[sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to further process each plot description by converting it to lower case, stripping leading and trailing white space, and then tokenizing by splitting on spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_tok = []\n",
    "for plot in plots:\n",
    "    processed = plot.lower().strip().split(' ')\n",
    "    plots_tok.append(processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we will remove tokens that have digits, have possessives or contractions, or are empty strings.\n",
    "- `is_numeric(string)` checks if `string` has any numbers\n",
    "- `has_poss_contr(string)` checks if `string` has possessives or contractions\n",
    "- `empty_string(string)` checks if `string` is an empty string\n",
    "- `remove_string(string)` checcks if `string` should be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_numeric(string):\n",
    "    for char in string:\n",
    "        if char.isdigit():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def has_poss_contr(string):\n",
    "    for i in range(len(string) - 1):\n",
    "        if string[i] == '\\'' and string[i+1] == 's':\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def empty_string(string):\n",
    "    return string == ''\n",
    "\n",
    "def remove_string(string):\n",
    "    return is_numeric(string) | has_poss_contr(string) | empty_string(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for plot in plots_tok:\n",
    "    filtered = []\n",
    "    for token in plot:\n",
    "        if not remove_string(token):\n",
    "            filtered.append(token)\n",
    "    temp.append(filtered)\n",
    "plots_tok = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that to build topic models, we require the following components:\n",
    "- A vocabulary of tokens that appear across all documents.\n",
    "- A mapping of those tokens to a unique integer identifier, because topic model algorithms treat words by these identifiers, and not the strings themselves. For example, we represent `'epidemic'` as `word2id['epidemic'] = 50`\n",
    "- The corpus, where each document in the corpus is a collection of tokens, where each token is represented by the identifier and the number of times it appears in the document. For example, in the first document above the token `'epidemic'`, which appears twice, is represented as `(50, 2)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will build a vocabulary representing the tokens that have appeared across all the plot descriptions we have. When doing this, we typically want to (1) remove rare words, (2) remove stop words and (3) stem/lemmatize words.\n",
    "\n",
    "Let's introduce the concept of stemming and lemmatization. Both are techniques to derive the root form of a word (`'run'`), given an inflected form of the word (`'running'`).\n",
    "\n",
    "**Stemming**. This is the process of shortening a word based on common heuristics, such as removing suffixes. For example, `'appeared'` -> `'appear'`. Sometimes, there is nothing to remove, like in `'saw'` -> `'saw'`.\n",
    "\n",
    "**Lemmatization**. This is the process of deriving the root word based on its part of speech and word morphology. For example, `'saw'` -> `'saw'` if it is used as a noun, but `'saw'` -> `'see'` if it is used as a verb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "temp = []\n",
    "for plot in plots_tok:\n",
    "    processed = []\n",
    "    for token in plot:\n",
    "        processed.append(lemmatizer.lemmatize(token, pos='v'))\n",
    "    temp.append(processed)\n",
    "plots_tok = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we can use the `Counter` class to build the vocabulary. The `Counter` is an extension of the Python dictionary, and also has key-value pairs. For the `Counter`, keys are the objects to be counted, while values are their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 33686\n"
     ]
    }
   ],
   "source": [
    "vocab = Counter()\n",
    "for plot in plots_tok:\n",
    "    vocab.update(plot)\n",
    "\n",
    "print(\"Number of unique tokens: %d\" % len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that removing rare words helps prevent our vocabulary from being too large. Many tokens appear only a few times across all the plot descriptions. Keeping them in the vocabulary increases subsequent computation time. Furthermore, their presence tends not to carry much significance for a document, since they can be considered as anomalies.\n",
    "\n",
    "We remove rare words by only keeping tokens that appear more than 25 times across all plot descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 5586\n"
     ]
    }
   ],
   "source": [
    "tokens = []\n",
    "for token in vocab.elements():\n",
    "    if vocab[token] > 25:\n",
    "        tokens.append(token)\n",
    "vocab = Counter(tokens)\n",
    "\n",
    "print(\"Number of unique tokens: %d\" % len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that stop words are defined as very common words such as `'the'` and `'a'`. Removing stop words is important because their presence also does not carry much significance, since they appear in all kinds of texts.\n",
    "\n",
    "We will remove stop words by removing the 200 most common tokens across all the plot descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 5386\n"
     ]
    }
   ],
   "source": [
    "stop_words = []\n",
    "for item in vocab.most_common(200):\n",
    "    stop_word = item[0]\n",
    "    stop_words.append(stop_word)\n",
    "tokens = []\n",
    "for token in vocab.elements():\n",
    "    if token not in stop_words:\n",
    "        tokens.append(token)\n",
    "vocab = Counter(tokens)\n",
    "\n",
    "print(\"Number of unique tokens: %d\" % len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a mapping for tokens to unique identifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens mapped: 5386\n",
      "Identifier for 'photograph': 2152\n",
      "Word for identifier 2152: photograph\n"
     ]
    }
   ],
   "source": [
    "items = vocab.items()\n",
    "id2word = {}\n",
    "word2id = {}\n",
    "idx = 0\n",
    "for word, count in vocab.items():\n",
    "    id2word[idx] = word\n",
    "    word2id[word] = idx\n",
    "    idx += 1\n",
    "    \n",
    "print(\"Number of tokens mapped: %d\" % len(id2word))\n",
    "print(\"Identifier for 'photograph': %d\" % word2id['photograph'])\n",
    "print(\"Word for identifier %d: %s\" % (word2id['photograph'], id2word[word2id['photograph']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will remove, for each plot description, the tokens that are not found in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for plot in plots_tok:\n",
    "    filtered = []\n",
    "    for token in plot:\n",
    "        if token in vocab:\n",
    "            filtered.append(token)\n",
    "    temp.append(filtered)\n",
    "plots_tok = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the corpus. Recall that for the tokens in a plot description:\n",
    "\n",
    "```\n",
    "['doctor', 'pearson', 'michael', 'rennie', 'canadian', 'hospital', 'province', 'quebec', 'receive', 'series', 'poison', 'pen', 'letter', 'letter', 'sign', 'mysterious', 'picture', 'feather', 'deliver', 'others', 'small', 'canadian', 'cora', 'laurent', 'constance', 'smith', 'main', 'doctor', 'dr', 'laurent', 'charles', 'boyer', 'hospital', 'receive', 'letter', 'accuse', 'affair', 'pearson', 'letter', 'inform', 'shell', 'shock', 'veteran', 'mr', 'cancer', 'distraught', 'commit', 'suicide', 'quickly', 'townsfolk', 'point', 'finger', 'possible', 'suspect'] \n",
    "```\n",
    "the corpus has the format\n",
    "```\n",
    "[(1841, 2), (2095, 2), (2096, 1), (2097, 1), (2098, 2), (105, 2), (2099, 1), (2100, 1), (270, 2), (1763, 1), (1870, 1), (2101, 1), (2017, 4), (633, 1), (1270, 1), (1093, 1), (2102, 1), (1197, 1), (113, 1), (1583, 1), (2103, 1), (2104, 2), (2105, 1), (873, 1), (1950, 1), (107, 1), (2106, 1), (2107, 1), (116, 1), (1436, 1), (62, 1), (2108, 1), (213, 1), (2109, 1), (1205, 1), (2110, 1), (1042, 1), (1275, 1), (1259, 1), (1342, 1), (2111, 1), (440, 1), (1662, 1), (374, 1), (663, 1)]\n",
    "```\n",
    "\n",
    "where each element is a pair containing the identifier for the token and the count of that token in just that plot description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot, tokenized:\n",
      " ['couple', 'apartment', 'announce', 'separation', 'shock', 'news', 'personally', 'very', 'hurt', 'confuse', 'dinner', 'few', 'weeks', 'apartment', 'colleague', 'together', 'opera', 'dinner', 'phone', 'someone', 'accuse', 'affair', 'marriage\\r\\n', 'introduce', 'girlfriend', 'trainer', 'shop', 'girlfriend', 'waitress', 'crazy', 'week', 'introduce', 'magazine', 'colleague', 'clearly', 'interest', 'herself', 'date', 'smite', 'relationship\\r\\n', 'develop', 'friendship', 'student', 'read', 'manuscript', 'novel', 'comment', 'several', 'react', 'party', 'someone', 'fly', 'jealous', 'rage', 'intense', 'argument', 'bed', 'marriage', 'chance', 'leave\\r\\n', 'than', 'weeks', 'however', 'together', 'couple', 'dinner', 'dinner', 'argument', 'share', 'poetry', 'fail', 'pass', 'think', 'relationship', 'week', 'birthday', 'party', 'music', 'box', 'present', 'kiss', 'though', 'share', 'romantic', 'moment', 'should', 'pursue', 'any', 'further', 'walk', 'rain', 'ruin', 'relationship', 'need', 'alone', 'cant', 'feel', 'hurt', 'walk', 'rain', 'beg', 'stay', 'year', 'half', 'audience', 'together', 'admit', 'marital', 'problems', 'exist', 'solve', 'accept', 'problems', 'simply', 'price', 'pay', 'remain', 'together\\r\\n', 'alone', 'date', 'hurt', 'anyone', 'immediate', 'cut', 'black', 'unseen', 'documentary', 'crew'] \n",
      "\n",
      "Plot, in corpus format:\n",
      " [(33, 2), (1915, 2), (1142, 1), (3261, 1), (569, 1), (502, 1), (2553, 1), (353, 1), (3301, 3), (3116, 1), (1749, 4), (1411, 1), (1459, 2), (3433, 2), (260, 3), (1912, 1), (2490, 1), (1476, 2), (625, 1), (1337, 1), (1347, 1), (129, 2), (1072, 2), (2247, 1), (1638, 1), (2734, 1), (4110, 1), (1960, 2), (1559, 1), (4160, 1), (454, 1), (611, 1), (3088, 2), (2471, 1), (3498, 1), (540, 1), (603, 1), (1621, 1), (257, 1), (4973, 1), (397, 1), (4288, 1), (581, 1), (3389, 1), (192, 2), (932, 1), (698, 1), (309, 1), (3765, 1), (2551, 2), (139, 1), (459, 1), (2113, 1), (3606, 1), (1235, 1), (532, 1), (1488, 2), (4158, 1), (463, 1), (81, 1), (369, 1), (615, 2), (1453, 1), (1988, 1), (119, 1), (146, 1), (299, 1), (1441, 1), (204, 1), (205, 1), (1396, 1), (478, 1), (794, 1), (677, 1), (34, 2), (2101, 2), (425, 1), (1013, 1), (1229, 2), (169, 1), (1048, 1), (213, 1), (1420, 1), (674, 1), (433, 1), (991, 1), (1798, 1), (3631, 1), (3113, 2), (3663, 1), (2629, 1), (281, 1), (489, 1), (1114, 1), (269, 1), (1136, 1), (3299, 1), (1093, 1), (3219, 1), (867, 1), (164, 1), (560, 1), (4678, 1), (188, 1)]\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for plot in plots_tok:\n",
    "    plot_count = Counter(plot)\n",
    "    corpus_doc = []\n",
    "    for item in plot_count.items():\n",
    "        pair = (word2id[item[0]], item[1])\n",
    "        corpus_doc.append(pair)\n",
    "    corpus.append(corpus_doc)\n",
    "\n",
    "print(\"Plot, tokenized:\\n\", plots_tok[sample], \"\\n\")\n",
    "print(\"Plot, in corpus format:\\n\", corpus[sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to create our topic model!\n",
    "\n",
    "We again use gensim, a Python library to create topic models. Also, we again use the algorithm called latent dirichlet allocation implemented in the gensim library. \n",
    "\n",
    "**This step takes about 40s**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.4 s, sys: 9.73 ms, total: 38.4 s\n",
      "Wall time: 38.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                            id2word=id2word,\n",
    "                                            num_topics=10, \n",
    "                                            random_state=100,\n",
    "                                            update_every=1,\n",
    "                                            chunksize=100,\n",
    "                                            passes=10,\n",
    "                                            alpha='auto',\n",
    "                                            per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After building the topic model, we want to view the 10 topics. The topics are represented as a combination of keywords with corresponding weight on the keyword. Note that the order of these topics can change between different training runs of the topic model, since there is no ordering between topics and gensim returns them in an arbitrary order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>word rank</th> <th>topic 0</th> <th>topic 1</th> <th>topic 2</th> <th>topic 3</th> <th>topic 4</th> <th>topic 5</th> <th>topic 6</th> <th>topic 7</th> <th>topic 8</th> <th>topic 9</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>1        </td> <td>officer</td> <td>win     </td> <td>power     </td> <td>village  </td> <td>marriage</td> <td>movie    </td> <td>gang    </td> <td>ship      </td> <td>battle    </td> <td>body    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2        </td> <td>case   </td> <td>team    </td> <td>destroy   </td> <td>sister   </td> <td>feel    </td> <td>accident </td> <td>company </td> <td>island    </td> <td>city      </td> <td>appear  </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>3        </td> <td>arrest </td> <td>college </td> <td>world     </td> <td>hospital </td> <td>how     </td> <td>girls    </td> <td>local   </td> <td>matter    </td> <td>rescue    </td> <td>enter   </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>4        </td> <td>dead   </td> <td>high    </td> <td>child     </td> <td>student  </td> <td>parent  </td> <td>dream    </td> <td>business</td> <td>project   </td> <td>land      </td> <td>open    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>5        </td> <td>though </td> <td>join    </td> <td>boy       </td> <td>doctor   </td> <td>very    </td> <td>rich     </td> <td>revenge </td> <td>sea       </td> <td>travel    </td> <td>search  </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>6        </td> <td>commit </td> <td>students</td> <td>children  </td> <td>villagers</td> <td>stay    </td> <td>act      </td> <td>brothers</td> <td>boat      </td> <td>defeat    </td> <td>chase   </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>7        </td> <td>leader </td> <td>members </td> <td>baby      </td> <td>spirit   </td> <td>together</td> <td>around   </td> <td>kidnap  </td> <td>million   </td> <td>encounter </td> <td>hide    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>8        </td> <td>truth  </td> <td>final   </td> <td>ghost     </td> <td>teacher  </td> <td>happen  </td> <td>character</td> <td>sell    </td> <td>wealth    </td> <td>human     </td> <td>throw   </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>9        </td> <td>former </td> <td>second  </td> <td>always    </td> <td>uncle    </td> <td>accept  </td> <td>perform  </td> <td>jail    </td> <td>crew      </td> <td>soldier   </td> <td>reach   </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>10       </td> <td>under  </td> <td>henchmen</td> <td>create    </td> <td>due      </td> <td>too     </td> <td>scene    </td> <td>law     </td> <td>launch    </td> <td>temple    </td> <td>inside  </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>11       </td> <td>suspect</td> <td>events  </td> <td>transform </td> <td>study    </td> <td>finally </td> <td>part     </td> <td>sisters </td> <td>passengers</td> <td>mysterious</td> <td>suddenly</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>12       </td> <td>witness</td> <td>game    </td> <td>everything</td> <td>form     </td> <td>think   </td> <td>real     </td> <td>phone   </td> <td>beach     </td> <td>capture   </td> <td>wait    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>13       </td> <td>war    </td> <td>base    </td> <td>bear      </td> <td>elder    </td> <td>change  </td> <td>whose    </td> <td>boss    </td> <td>treasure  </td> <td>army      </td> <td>trap    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>14       </td> <td>claim  </td> <td>fail    </td> <td>forest    </td> <td>teach    </td> <td>never   </td> <td>music    </td> <td>crime   </td> <td>ocean     </td> <td>control   </td> <td>water   </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>15       </td> <td>secret </td> <td>series  </td> <td>person    </td> <td>past     </td> <td>good    </td> <td>different</td> <td>threaten</td> <td>advantage </td> <td>form      </td> <td>catch   </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_topics = 10\n",
    "num_words = 15\n",
    "top_words = Table().with_column('word rank', np.arange(1,num_words+1))\n",
    "for k in np.arange(num_topics): \n",
    "    topic = lda_model.get_topic_terms(k, num_words)\n",
    "    words = [id2word[topic[i][0]] for i in np.arange(num_words)]\n",
    "    probs = [topic[i][1] for i in np.arange(num_words)]\n",
    "    top_words = top_words.with_column('topic %d' % k, words)\n",
    "    \n",
    "top_words.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the probability distribution for a given plot description in the `corpus`. This represents how likely it is for the plot description to belong to each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.0799971),\n",
       " (1, 0.083626099),\n",
       " (2, 0.040867072),\n",
       " (3, 0.034000255),\n",
       " (4, 0.52988881),\n",
       " (5, 0.081525929),\n",
       " (6, 0.06373515),\n",
       " (7, 0.010502912),\n",
       " (8, 0.024644673),\n",
       " (9, 0.051212043)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 3402\n",
    "lda_model.get_document_topics(corpus[sample], minimum_probability=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's represent this as a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Topic</th> <th>Probabilities</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>0    </td> <td>0.0799951    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1    </td> <td>0.0836254    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2    </td> <td>0.0408732    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>3    </td> <td>0.0340003    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>4    </td> <td>0.529889     </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>5    </td> <td>0.0815253    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>6    </td> <td>0.0637311    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>7    </td> <td>0.0105029    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>8    </td> <td>0.0246447    </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>9    </td> <td>0.0512127    </td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic with highest probability: 4 (0.529889)\n"
     ]
    }
   ],
   "source": [
    "topic_dist = lda_model.get_document_topics(corpus[sample], minimum_probability=0)\n",
    "topics = [pair[0] for pair in topic_dist]\n",
    "probabilities = [pair[1] for pair in topic_dist]\n",
    "topic_dist_table = Table().with_columns('Topic', topics, 'Probabilities', probabilities)\n",
    "topic_dist_table.show(20)\n",
    "t = np.argmax(probabilities)\n",
    "print(\"Topic with highest probability: %d (%f)\" % (t, probabilities[t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now as a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(11,4)\n",
    "plt.bar(topic_dist_table.column('Topic'), topic_dist_table.column('Probabilities'), align='center', alpha=1, color='salmon')\n",
    "plt.xlabel('topic')\n",
    "plt.ylabel('probability')\n",
    "plt.title('Per Topic Probability Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems the movie plot description\n",
    "\n",
    "```\n",
    "The film is about two couples: Jack (Pollack) and Sally (Davis), and Gabe (Allen) and Judy (Farrow). The film starts when Jack and Sally arrive at Gabe and Judy's apartment and announce their separation. Gabe is shocked, but Judy takes the news personally and is very hurt. Still confused, they go out for dinner at a Chinese restaurant.\n",
    "A few weeks later Sally goes to the apartment of a colleague. They plan to go out together to the opera and then to dinner. Sally asks if she can use his phone, and calls Jack. Learning from him that he has met someone, she accuses him of having had an affair during their marriage.\n",
    "Judy and Gabe are introduced to Jack's new girlfriend, Sam, an aerobics trainer. While Judy and Sam shop, Gabe calls Jack's new girlfriend a \"cocktail waitress\" and tells him that he is crazy for leaving Sally for her. About a week later, Judy introduces Sally to Michael (Neeson), Judy's magazine colleague who she clearly is interested in herself. Michael asks Sally out, and they begin dating; Michael is smitten, but Sally is dissatisfied with the relationship.\n",
    "Meanwhile, Gabe has developed a friendship with a young student of his, Rain, and has her read the manuscript of his novel. She comments on its brilliance, but has several criticisms, to which Gabe reacts defensively.\n",
    "At a party, Jack learns from a friend that Sally is seeing someone, and flies into a jealous rage. He and Sam break up after an intense argument, and Jack drives back to his house to find Sally in bed with Michael. He asks Sally to give their marriage another chance, but she tells him to leave.\n",
    "Less than two weeks later, however, Jack and Sally are back together and the couple meet Judy and Gabe for dinner like old times. After dinner, Judy and Gabe get into an argument about her not sharing her poetry. After Gabe makes a failed pass at her, Judy tells him she thinks the relationship is over; a week later Gabe moves out. Judy begins seeing Michael.\n",
    "Gabe goes to Rain's 21st birthday party, and gives her a music box as a present. She asks him to kiss her, and though the two share a romantic moment, Gabe tells her they should not pursue it any further. As he walks home in the rain, he realizes that he has ruined his relationship with Judy.\n",
    "Michael tells Judy he needs time alone, then says he can't help still having feelings for Sally. Angry and hurt, Judy walks out into the rain. Highlighting her \"passive aggressiveness,\" Michael follows and begs her to stay with him. A year and a half later they marry.\n",
    "At the end, the audience sees a pensive Jack and Sally back together. Jack and Sally admit their marital problems still exist (her frigidity is not solved), but they find they accept their problems as simply the price they have to pay to remain together.\n",
    "Gabe is living alone because he says he is not dating for the time being, as he does not want to hurt anyone. The film ends with an immediate cut to black after Gabe asks the unseen documentary crew, \"Can I go? Is this over?\"\n",
    "```\n",
    "\n",
    "has the greatest likelihood to fall under the topic number with topic relating to relationships, which matches our intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Models for TED Talk Transcripts\n",
    "\n",
    "*Your turn!* Your task is broken down into two parts.\n",
    "\n",
    "#### 1. Run Topic Models\n",
    "You are given a new dataset of transcripts of TED Talks. This dataset is also [obtained](https://www.kaggle.com/rounakbanik/ted-talks) from Kaggle. You can use the following link to load the data, in a CSV file `ted_talks.csv`, from s3:\n",
    "\n",
    "```\n",
    "filename = \"https://s3.amazonaws.com/sds171/labs/lab07/ted_talks.csv\"\n",
    "data = Table.read_table(filename)\n",
    "```\n",
    "\n",
    "Run the code above to train a topic model over this new dataset. In particular,\n",
    "- Load the CSV file. You can simply use the two lines of code above, without skipping any rows.\n",
    "- Preprocess\n",
    "    - Discard some transcripts if you find it appropriate.\n",
    "    - Add a regular expression here if you find it is appropriate.\n",
    "    - Tokenize and remove numerics, possessives/contractions, and empty strings\n",
    "    - Lemmatize tokens\n",
    "- Create a vocabulary using a `Counter`\n",
    "- Filter the vocabulary by removing rare words and stop words. You should be getting a final vocabulary size of about 7000.\n",
    "- Create the identifier mappings `word2id` and `id2word`\n",
    "- Filter tokens from the tokenized data using the final vocabulary\n",
    "- Create the corpus\n",
    "- Train the topic model\n",
    "- Show the topics along with the top words\n",
    "\n",
    "You should find yourself simply using the code given above for this new dataset. Very few changes are necessary!\n",
    "\n",
    "\n",
    "#### 2. Discussion\n",
    "Discuss the results. Choose two or three articles and describe how the most probable topic does or does not seem to accurately represent the main theme of the paper. Include your comments in Markdown cell, with code cells added as needed to pull out particular rows of your table. You may find it useful to copy over the code for `create_topic_table` from the previous lab."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
