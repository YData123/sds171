{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 09: Word embeddings (2/2)\n",
    "\n",
    "<br>\n",
    "\n",
    "<img width=300 align=\"left\" margin=\"20\" src=\"number-matrix.jpg\">\n",
    "<img width=300 margin=\"20\" src=\"wordgeom.png\">\n",
    "\n",
    "<br>\n",
    "\n",
    "In this lab we will continue to work with word embeddings&mdash;machine learning algorithms that represent categorical data like words as vectors in a high dimensional space. \n",
    "\n",
    "Embeddings are constructed using cooccurrence statistics, and can be applied whenever you have \"objects\" that appear together&mdash;songs in playlists, words in documents, ingredients in recipes... Embeddings can reveal surprising semantic relations encoded in linear relationships; but they are \"data hungry\" and require large corpora of text or other coocurrence data to construct useful representations.\n",
    "\n",
    "We will first explore how to visualize the embedding vectors. Perhaps confusingly, this is done by embedding the embedding vectors themselves into two dimensions. The most \n",
    "popular method for doing this is called t-SNE (\"TEE snee\"). [Here](https://lvdmaaten.github.io/tsne/) is an overview of t-SNE by its creator, and [here](http://cs.stanford.edu/people/karpathy/tsnejs/) is an introduction that includes some (potentially) interesting examples. We will then explore ways in which embeddings may represent certain \"cultural biases,\" which has been a topic of recent interest in the machine learning and AI communities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by loading in the 100-dimensional GloVe embeddings we worked with last time. Recall that these are trained using word cooccurrence counts from about 6 billion words of text from Wikipedia pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as gdl\n",
    "from gensim.models import KeyedVectors\n",
    "glove = gdl.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll import the usual modules, and in addition a module that implements t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datascience import *\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now define two functions. The first takes a pre-trained embedding model, together with a vocabulary of words (as strings), which is a subset of the words in the embedding vocabulary. It then runs t-SNE on that vocabulary, displaying the results as a point cloud. The second function displays the location of particular words, so that we can get a sense of the geometry of the embeddings, and what's close to what."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_tsne_for_voc(model, voc):\n",
    "    word_list = [w for w in model.vocab]\n",
    "    w = word_list[0]\n",
    "    dim = len(model[w])\n",
    "    print(\"embedding dimension: %d\" % dim)\n",
    "    print(\"computing t-SNE vectors over %d words...\" % len(voc))\n",
    "    \n",
    "    # collect all the vectors in a list\n",
    "    arr = np.empty((0,dim), dtype='f')\n",
    "    for w in voc:\n",
    "        wrd_vector = model[w]\n",
    "        arr = np.append(arr, np.array([wrd_vector]), axis=0)\n",
    "        \n",
    "    # find tsne coords for 2 dimensions\n",
    "    tsne = TSNE(n_components=2, random_state=0)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    Y = tsne.fit_transform(arr)\n",
    "\n",
    "    # display scatter plot of the 2-d vectors\n",
    "    plt.figure(figsize=(15,15))    \n",
    "    plt.scatter(Y[:,0], Y[:,1], s=2)\n",
    "    plt.show()\n",
    "    return (Y,voc)\n",
    "\n",
    "\n",
    "# this function displays the \"word cloud\" of all points from the t-SNE vectors,\n",
    "# and then labels the words in the array input_words\n",
    "\n",
    "def display_tsne_words(Z, input_words, size1=2, size2=50, offset=5):\n",
    "    Y = Z[0]\n",
    "    voc = Z[1]\n",
    "    x_coord = Y[:, 0]\n",
    "    y_coord = Y[:, 1]\n",
    "    # display scatter plot\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.scatter(x_coord, y_coord, s=size1)\n",
    "    for w in np.arange(len(input_words)):\n",
    "        index = voc.index(input_words[w])\n",
    "        plt.scatter(x_coord[index], y_coord[index],s=size2)\n",
    "        plt.annotate(input_words[w], xy=(x_coord[index],y_coord[index]), \\\n",
    "                     xytext=(offset,offset), textcoords='offset points')\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following bit of code computes word frequencies. We read in the 'text8' dataset from last lab, which you'll recall is a few million words of Wikipedia text. We then build a vocabulary over this data using the `Counter` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import time\n",
    "import nltk\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "logging.root.level = logging.CRITICAL \n",
    "\n",
    "s = time.time()\n",
    "voc = Counter()\n",
    "for rec in open ('../lab08/text8', 'r'):\n",
    "    rec = rec.strip()\n",
    "    voc.update(rec.split())\n",
    "print(time.time()-s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we grab the most common words, excluding the top (say) 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5000\n",
    "words = voc.most_common(n+100)[100:]\n",
    "vocab = [word[0] for word in words]\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run t-SNE on the GloVe embeddings, but restricting to this subset of the model vocabulary. This is only for speed and memory considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "filtered_voc = [w for w in vocab if w in glove.vocab]\n",
    "len(filtered_voc)\n",
    "plt.figure(figsize=(15,15))\n",
    "Z = generate_tsne_for_voc(glove, filtered_voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we display some words by labeling points in the t-SNE embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_tsne_words(Z, input_words=['conservative', 'liberal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn!\n",
    "\n",
    "Using the functions defined above, generate several plots show locations of points you choose. Generate a plot, and then describe in a markdown cell why this either makes sense or is puzzling to you. For example, you might find that \"apple\" is not near \"fruit\" but is rather close to \"ibm.\" This should be easy to explain. Find others and comment on what you see.\n",
    "\n",
    "To start, first generate t-SNE vectors for a larger subset of the 400,000 model vocabulary. You might use something like `n=10000`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
